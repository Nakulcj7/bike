{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nakulcj7/bike/blob/main/Bike_sharing_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -Seoul Bike Sharing Demand Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    -Regression\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bike sharing systems have gained widespread popularity in urban environments, offering a sustainable and efficient mode of transportation. This project focuses on developing a predictive model for bike sharing demand, leveraging historical data, weather conditions, and other relevant factors. The primary goal is to create a robust and accurate prediction system to optimize bike allocation and enhance user experience.There were approximately 8760 records and 14 attributes in the dataset.This dataset contains information on Seoul city's weather conditions (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall)and the number of bikes rented on every hour and the date information."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Nakulcj7/bike/blob/main/Bike_sharing_prediction.ipynb\n"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "It is necessary to make the rental bike avaiable and accessible for the public at the right time as the waiting period shortens.Eventually,providing the city with a stable supply of rental bikes becomes a major concern.The main think to focus here is to predict the bike count required at each hour for a stable supply of rental bikes.\n",
        "\n",
        "\n",
        "The major objective here is to count the rental bikes required on an daily hour basis and also to identify the features which influences the hourly demant for rental bikes."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the dataset\n",
        "bd_df=pd.read_csv(\"/content/drive/MyDrive/Almabetter/SeoulBikeData.csv\", encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "oiY18hAKfd2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "bd_df.head(5)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "bd_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'number of rows : {bd_df.shape[0]}  \\nnumber of columns : {bd_df.shape[1]}')"
      ],
      "metadata": {
        "id": "q9bgooJNiIvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "bd_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#viewing the statistical summary of the data\n",
        "bd_df.describe(include='all').T"
      ],
      "metadata": {
        "id": "NgLRP7C6ioVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "bd_df[bd_df.duplicated()]"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows that there are no duplicate entries in the dataset.\n"
      ],
      "metadata": {
        "id": "fHOc-sBEjHqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "bd_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it is evident that there is no null values in the dataset.So we can that the dataset is balanced.\n"
      ],
      "metadata": {
        "id": "UWfMp_x1jkxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset provided contains 14 columns and 8760 rows and does not have any missing or duplicate values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "bd_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "bd_df.describe(include='all').T\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This dataset contains information on Seoul city's weather conditions (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall)and the number of bikes rented on every hour and the date information.\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "\n",
        "*   Date :The date of each observation in the format 'year-month-day'\n",
        "\n",
        "*   Rented Bike count - Count of bikes rented at each hour\n",
        "\n",
        "*   Hour - Hour of the day\n",
        "\n",
        "*   Temperature - Temperature recorded in the city in Celsius (°C).\n",
        "\n",
        "*   Humidity - Relative humidity in %\n",
        "\n",
        "*   \n",
        "Windspeed - Speed of the wind in m/s\n",
        "\n",
        "\n",
        "*   Visibility - measure of distance at which object or light can be clearly discerned in units of 10m\n",
        "*   Dew point temperature - Temperature recorded in the beginning of the day in Celsius(°C).\n",
        "\n",
        "\n",
        "*   Solar radiation - Intensity of sunlight in MJ/m^2\n",
        "\n",
        "\n",
        "*   Rainfall - Amount of rainfall received in mm\n",
        "\n",
        "\n",
        "*   Snowfall - Amount of snowfall received in cm\n",
        "\n",
        "\n",
        "*   Seasons - Season of the year (Winter, Spring, Summer, Autumn)\n",
        "\n",
        "\n",
        "*   Holiday - Whether the day is a Holiday or not (Holiday/No holiday)\n",
        "\n",
        "\n",
        "*   Functional Day -Whether the rental service is available (Yes-Functional hours) or not (No-Non functional hours)\n",
        "\n"
      ],
      "metadata": {
        "id": "QIuQFZnOp5-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "bd_df.nunique()\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "#Converting the datatype of Date column to datatime\n",
        "bd_df['Date'] = pd.to_datetime(bd_df['Date'], format='%d/%m/%Y')\n",
        "\n",
        "#Extracting Month,Weekday and Year from the date column\n",
        "bd_df['Month']=bd_df['Date'].dt.month\n",
        "bd_df['Days_of_week']=bd_df['Date'].dt.day_name()\n",
        "bd_df['Year']=bd_df['Date'].dt.year\n",
        "bd_df['Day']=bd_df['Date'].dt.day\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The number of unique values in Date column\n",
        "bd_df['Date'].nunique()"
      ],
      "metadata": {
        "id": "gsajp_VC6kyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains records of rented bikes per hour for a period of 365 days"
      ],
      "metadata": {
        "id": "GuNm6DH-6scU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The number of unique values in Year column\n",
        "bd_df['Year'].value_counts()\n"
      ],
      "metadata": {
        "id": "s9THVVxB600O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the records are from the year 2018"
      ],
      "metadata": {
        "id": "UdrHfB2y62pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the date of first and last entry in the dataset\n",
        "print(f'The dataset contains observations from ',min(bd_df['Date']).date(),'to',max(bd_df['Date']).date())\n"
      ],
      "metadata": {
        "id": "8HoC1F8j6_fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a column which specifies  if the day is a Weekend('Y')or not ('N')\n",
        "bd_df['Weekend']=bd_df['Days_of_week'].apply(lambda x : ('Y') if x in ['Saturday','Sunday'] else ('N'))\n"
      ],
      "metadata": {
        "id": "GX_Tr4G27Bof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the unique values in the categorical columns\n",
        "categorical_columns=['Seasons','Holiday', 'Functioning Day','Days_of_week','Weekend']\n",
        "\n",
        "for col in categorical_columns:\n",
        "  print(f'The unique values in the column {col} are {bd_df[col].unique()}')"
      ],
      "metadata": {
        "id": "rNMFEvHn7Iqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Univariate Analysis**"
      ],
      "metadata": {
        "id": "iWn2Ptxy72sR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how some of the important numerical independent features are distibuted in our data."
      ],
      "metadata": {
        "id": "aY5OPq0Y8Yj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12,15))\n",
        "c=1\n",
        "lis=['Hour','Temperature(°C)',\t'Humidity(%)',\t'Wind speed (m/s)',\t'Visibility (10m)',\t'Dew point temperature(°C)',\t'Solar Radiation (MJ/m2)',\t'Rainfall(mm)','Snowfall (cm)']\n",
        "for i in lis:\n",
        "  plt.subplot(3,3, c)\n",
        "  sns.histplot(bd_df[i],kde=True)\n",
        "  plt.title('Distibution of {}'.format(i))\n",
        "  c+=1\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "g968hb7P8faT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Distribution of Temperature,Humidity,Dew point temperature are almost normal.\n",
        "\n",
        "*   Wind speed,Solar Radiation,Rainfall,Snowfall-positively skewed\n",
        "*   Visibility is negatively skewed\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-tKaqbGv80E6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets see how is the dependent variable Rented Bike Count distributed?"
      ],
      "metadata": {
        "id": "6poiM7Nv9QjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(bd_df['Rented Bike Count'],kde=True,color='black')\n",
        "plt.title('Distibution of Rented Bike Count')"
      ],
      "metadata": {
        "id": "CR6_XqR-9c8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for outliers\n",
        "\n",
        "fig = plt.figure(figsize=(8,25))\n",
        "c=1\n",
        "for i in lis :\n",
        "    plt.subplot(13,1, c)\n",
        "    plt.xlabel('Distibution of {}'.format(i))\n",
        "    sns.boxplot(x=i,data=bd_df)\n",
        "    c = c + 1\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n"
      ],
      "metadata": {
        "id": "kqBZEBRh9pjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The outlier values are not extreme,nor unusual.So,we retain these values in our dataset."
      ],
      "metadata": {
        "id": "_kBx6jRQ9xrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The number of records belonging to each category\n",
        "for col in categorical_columns:\n",
        " print('Column :',col)\n",
        " print(bd_df[col].value_counts(),'\\n')"
      ],
      "metadata": {
        "id": "SQDIrY3x9zMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic conclusion from inivariate analysis\n",
        "\n",
        "\n",
        "*   Number of records are mostly similar throughout the seasons(need to dig more for better understanding).\n",
        "\n",
        "*   More number of records on non-holiday(working days) & Functioning days of the rental service.\n",
        "\n",
        "*   Bike rentals are fewers on Weekends\n",
        "\n",
        "*   Not much info from hour at the moment.\n",
        "*   The temperature is mostly >0, for now lets consider Seoul on the warmer side.\n",
        "\n",
        "\n",
        "*   Humidity is also moderate but still on warmer side.\n",
        "\n",
        "\n",
        "*   Wind speed is not that extreme.\n",
        "\n",
        "\n",
        "*   Most of the rainfall is <4 mm.\n",
        "\n",
        "*   Snowfall is mostly 0-1 cm and not that extreme in most cases.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E4P0ibni98CL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bivariate Analysis\n",
        "\n",
        "## Correlation Heatmap"
      ],
      "metadata": {
        "id": "LY2AajSA_Uu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Correlation heatmap of numerical features in the dataset\n",
        "plt.figure(figsize = (10,10))\n",
        "sns.heatmap(bd_df.corr(),annot=True,linewidth = 0.5, vmin=-1, vmax=1, cmap = 'YlGnBu')\n"
      ],
      "metadata": {
        "id": "6f0UbGklAGTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Dew point temperature is strongly correlated with temperature.\n",
        "*   Temperature,Hour shares a stronger correlation with Rented Bike count.\n",
        "\n"
      ],
      "metadata": {
        "id": "cfY8tkYEqVc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scatter plot showing the high correlation of Temperature and Dew point Temperature**"
      ],
      "metadata": {
        "id": "ckTqdoUyqoF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[10,6])\n",
        "sns.scatterplot(data=bd_df, y='Temperature(°C)', x='Dew point temperature(°C)')"
      ],
      "metadata": {
        "id": "R3LUDhJZq3E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Were rental services offered on non-functional days?**"
      ],
      "metadata": {
        "id": "_dJOV4wyq_KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(bd_df[bd_df['Functioning Day']=='No'])\n"
      ],
      "metadata": {
        "id": "4ZEdL7NJrJEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is highly unlikely that services will be provided on non-functional days.But since there were few observations (295) recorded on those days,let's check if there were any exceptional cases."
      ],
      "metadata": {
        "id": "QO6PEh-ErOHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the count of rental bikes,number of holidays &non-holidays and number of records for Functioning and Non-Functioning days\n",
        "\n",
        "bd_df.groupby(['Functioning Day','Holiday']).agg(bikerentalcounts=('Rented Bike Count','sum'),no_of_holidays_nonholidays=('Date',lambda x: x.nunique()),no_of_records=(('Date','count')))"
      ],
      "metadata": {
        "id": "qrR1E8F5rSXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(x='Functioning Day',y='Rented Bike Count',data=bd_df)"
      ],
      "metadata": {
        "id": "muIQGyhksywc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The rental service were functional on most days during the period from Dec 2017 to Nov 2018(only 13 non-functional days)\n",
        "*   Although,we've observed few records on Non-Functioning Day,rental services were not offered on those days(no exceptions)\n",
        "\n"
      ],
      "metadata": {
        "id": "cOVko2Yqs9Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Which are the days on which the rental facility was unavailable?**"
      ],
      "metadata": {
        "id": "6otkoHeGtLE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_functioning_days =bd_df.loc[bd_df['Functioning Day']=='No']\n",
        "\n",
        "#Holiday on which the rental service was unavailable\n",
        "non_functioning_days.loc[non_functioning_days['Holiday']=='Holiday']['Date'].unique()"
      ],
      "metadata": {
        "id": "IuKclgWZtVcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The holiday on which the rental service was not functioning is Hangeul day.It is a national Korean commemorative day marking the invention and the proclamation of Hangul , the alphabet of the Korean language"
      ],
      "metadata": {
        "id": "qp1Tn0omtjwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_functioning_days.loc[non_functioning_days['Holiday']=='No Holiday']['Date'].value_counts().to_frame(name = 'Hours_of_non_operation').reset_index().rename(columns={'index':'Date'})"
      ],
      "metadata": {
        "id": "6NwKtyWItsv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The services were not for available for 1 day in the month of April,1 day in May,4 days in September,3 days each in October and November."
      ],
      "metadata": {
        "id": "7ImTcIv_t1YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is the likelihood of people renting bikes on holidays and non-holidays?**"
      ],
      "metadata": {
        "id": "R3fSFUCft4pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(x='Holiday',y='Rented Bike Count',data=bd_df,palette='Set1')"
      ],
      "metadata": {
        "id": "-RTZWyquuDXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The demand for rented bikes is higher on non holidays\n",
        "\n"
      ],
      "metadata": {
        "id": "D9Z1KcWvuMHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is the count of rented bikes during different seasons over the entire period of observation?**"
      ],
      "metadata": {
        "id": "rwPx1coHujrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the total number of bikes rented in each season\n",
        "season_df=bd_df.groupby('Seasons')['Rented Bike Count'].sum().reset_index()['Rented Bike Count'].to_frame(name = 'season_count').reset_index()"
      ],
      "metadata": {
        "id": "or1I73rcusuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the total number of bikes rented in each month\n",
        "month_df=(bd_df.groupby(['Seasons','Month'])['Rented Bike Count'].sum()).to_frame(name = 'month_count').reset_index()"
      ],
      "metadata": {
        "id": "n0DCmtY5u14m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import calendar\n",
        "\n",
        "# Define month names\n",
        "month_names = [\n",
        "    'January', 'February', 'March', 'April', 'May', 'June',\n",
        "    'July', 'August', 'September', 'October', 'November', 'December'\n",
        "]\n",
        "\n",
        "# Create a figure and axis\n",
        "fig, ax = plt.subplots()\n",
        "size = 1\n",
        "group_names = ['Autumn', 'Spring', 'Summer', 'Winter']\n",
        "group_size = season_df['season_count']\n",
        "subgroup_size = month_df['month_count']\n",
        "\n",
        "# Setting figure colors using color maps\n",
        "a, b, c, d = [plt.cm.Blues, plt.cm.Reds, plt.cm.Greens, plt.cm.Purples]\n",
        "outer_colors = [a(.8), b(.8), c(.8), d(.8)]\n",
        "inner_colors = [*a(np.linspace(.7, .4, 3)), *b(np.linspace(.7, .4, 3)), *c(np.linspace(.7, .4, 3)), *d(np.linspace(.7, .4, 3))]\n",
        "\n",
        "# Creating the outer pie chart\n",
        "patches, texts, pcts = ax.pie(\n",
        "    group_size,\n",
        "    radius=3.2,\n",
        "    colors=outer_colors,\n",
        "    wedgeprops=dict(width=size, edgecolor='w'),\n",
        "    labels=group_names,\n",
        "    autopct='%1.1f%%',\n",
        "    textprops={'fontsize': 16},\n",
        "    labeldistance=1.1,\n",
        "    pctdistance=0.85\n",
        ")\n",
        "plt.setp(pcts, color='white', fontweight='bold')\n",
        "plt.setp(texts, fontweight=600)\n",
        "\n",
        "# Creating the inner pie chart with month names\n",
        "subgroup_names = [calendar.month_abbr[month_num] for month_num in month_df['Month']]\n",
        "patches1, texts1, pcts1 = ax.pie(\n",
        "    subgroup_size,\n",
        "    radius=3.2 - size,\n",
        "    colors=inner_colors,\n",
        "    labels=subgroup_names,\n",
        "    wedgeprops=dict(width=1.2, edgecolor='w'),\n",
        "    autopct='%1.1f%%',\n",
        "    textprops={'fontsize': 14},\n",
        "    labeldistance=0.8,\n",
        "    pctdistance=0.65\n",
        ")\n",
        "plt.setp(pcts1, color='w', fontweight='bold', fontsize=12)\n",
        "plt.setp(texts1, fontweight=600)\n",
        "\n",
        "ax.set(aspect=\"equal\")\n",
        "\n",
        "# Show the pie chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PwiKrefn3pkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The demand for rental bikes is lowest during Winters(Dec-Feb),highest during Summers(June-August)\n",
        "\n"
      ],
      "metadata": {
        "id": "_lsGNg3w4I1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is the demand for rental bikes during different days of the week?**"
      ],
      "metadata": {
        "id": "OhzuG6nM4RCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.boxenplot(x='Days_of_week',y='Rented Bike Count',data=bd_df,palette='Set1')"
      ],
      "metadata": {
        "id": "8zDP_gTA4hN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Least demand on Sunday,Slightly higher demand on Friday\n",
        "\n",
        "*   More demand on weekdays than weekends.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cEbBt4Xb4kmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the demand for rental bikes during weekdays and weekends?"
      ],
      "metadata": {
        "id": "tIlg6H__47SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(x='Weekend',y='Rented Bike Count',data=bd_df,palette='Set1')"
      ],
      "metadata": {
        "id": "WGqURdwz40AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bd_df.groupby(['Weekend'])['Rented Bike Count'].mean()"
      ],
      "metadata": {
        "id": "ZYFZleJ95G01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The average demand for rental bikes is lower on Weekends(Sat-Sun) as compared to Weekdays(Mon-Fri).\n",
        "\n"
      ],
      "metadata": {
        "id": "FrtzU83V5InX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the demand for rental bikes during different hours of the day?"
      ],
      "metadata": {
        "id": "AUDkNS7z5UDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "sns.lineplot(x='Hour',y='Rented Bike Count',data=bd_df,palette='Set1',hue='Seasons',lw=1.5)"
      ],
      "metadata": {
        "id": "pvGd8Egi5Wju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The demand for rental bikes peaks at 8 (8:00 am) and 18 (6:00 PM ).\n",
        "\n",
        "*   This peak in demand coincides with opening and closing hours of various institutions and offices.\n",
        "\n",
        "*   The demand for rental bikes increases steadily after 10:00 AM and continues till 6:00 PM\n",
        "*   The demand for bikes is least during the early hours (1:00 AM to 6:00 AM)\n",
        "\n",
        "*   Regardless,of the seasons,this has been the general trend noticed.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h9vcRiWH5hAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is the variation of Rented bikes count over the entire period of observation based on various factors?"
      ],
      "metadata": {
        "id": "lVIOX_fd6JW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(15,12))\n",
        "c=1\n",
        "cont = ['Date','Temperature(°C)',\t'Wind speed (m/s)',\t'Visibility (10m)',\t'Solar Radiation (MJ/m2)',\t'Rainfall(mm)',\t'Snowfall (cm)']\n",
        "for i in cont:\n",
        "  plt.subplot(4,2,c)\n",
        "  sns.lineplot(x=i,y='Rented Bike Count',data=bd_df,palette='inferno')\n",
        "  plt.title('Demand of Rental bikes at different {}'.format(i))\n",
        "  c = c + 1\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "5seeE6_l6N0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Temperature vs Bike count plot : The demand is higher during warmer temperatures (25°C-30°C)\n",
        "\n",
        "\n",
        "\n",
        "*  Windspeed vs Bike count plot : The demand for rental bikes is relatively uniform over all windspeeds upto 5 m/s .Beyond that speed,we observe a higher demand for bikes.\n",
        "\n",
        "*   Visibility vs Bike count plot : The count of bikes rented is few on times when the visibility is extremely low,less than 1000m.\n",
        "*   Solar radiation vs Bike count plot:There is an overall increase in the demand with increase in Solar radiation.\n",
        "\n",
        "\n",
        "*   Rainfall vs Bike count plot : The peak between 20 mm and 25 mm seems out of place,on refering to the dataset we find that such observations are recorded during Summer Season.However,people still continue to opt for rental bikes,since they have to go to work (No Holiday).\n",
        "\n",
        "\n",
        "*   Snowfall vs Bike count plot : The demand for bikes is comparatively lower when the snowfall received is 4 cm and above\n",
        "\n"
      ],
      "metadata": {
        "id": "m81Ajkvx6fyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspecting the observations where there is a peak in demand for bikes regardless of the weather conditions"
      ],
      "metadata": {
        "id": "2i1oBvt87Kwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.Rainfall\n",
        "bd_df[(bd_df['Rainfall(mm)'] >=20) & (bd_df['Rainfall(mm)'] <=25)]"
      ],
      "metadata": {
        "id": "-QQRPCRq7PHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are working days\n",
        "\n"
      ],
      "metadata": {
        "id": "i8BhcrR37VKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bd_df[(bd_df['Snowfall (cm)'] >=5) & (bd_df['Snowfall (cm)'] <=8)]"
      ],
      "metadata": {
        "id": "V12rRh7X7n4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   These are also working days\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P2keUylJ7unD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the factors which influence the demand for rental bikes during a day?"
      ],
      "metadata": {
        "id": "vPa0U7z97iOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig = plt.figure(figsize=(11,9))\n",
        "c=1\n",
        "columns=['Rented Bike Count','Temperature(°C)','Visibility (10m)', 'Wind speed (m/s)','Solar Radiation (MJ/m2)', 'Humidity(%)']\n",
        "for i in columns :\n",
        "    plt.subplot(3,2,c)\n",
        "    plt.ylabel(i)\n",
        "    plt.title(label=i,fontsize=15,color=\"green\")\n",
        "    sns.lineplot(data=bd_df, x='Hour', y=i, color='r')\n",
        "    c = c + 1\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "jBySSyix7eSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Temperature, visibility, windspeed, and humidity appear to be positively associated to the hourly demand for rental bikes.\n",
        "*   The rented bike counts are highest during the hours from 7:00 AM to 20.00 (8:00 PM), when the temperature is highest, there is the most visibility, windspeed, and humidity is lowest.\n",
        "\n"
      ],
      "metadata": {
        "id": "Pa6TMqrz79ZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the factors which influence the demand for rental bikes during different months?"
      ],
      "metadata": {
        "id": "ytOPY4rU8OHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12,12))\n",
        "c=1\n",
        "columns=['Rented Bike Count','Temperature(°C)','Visibility (10m)', 'Wind speed (m/s)','Solar Radiation (MJ/m2)', 'Humidity(%)','Rainfall(mm)','Snowfall (cm)']\n",
        "for i in columns :\n",
        "    plt.subplot(4,2, c)\n",
        "    plt.ylabel(i)\n",
        "    plt.title(label=i,fontsize=15,color=\"green\")\n",
        "    sns.lineplot(data=bd_df, x='Month', y=i, color='r')\n",
        "    c = c + 1\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "oJh2O4vA8SN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The monthly count of rented bikes is positively associated with Temperature.\n",
        "\n",
        "*   Snowfall movement coincides with season, with heavy snowfall from December to February throughout the winter season. There's a decline in count of rented bikes during these months.\n",
        "*   Rainfall tends to be more frequent in Seoul from June to August, during the summer season.However,this has not lead to decline in demand for rental bikes during those months.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y5LBGWfB8aoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the factors which influence the demand for rental bikes during various seasons of the year?"
      ],
      "metadata": {
        "id": "PNeJasTo8sEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12,12))\n",
        "c=1\n",
        "columns=['Rented Bike Count','Temperature(°C)','Visibility (10m)', 'Wind speed (m/s)', 'Humidity(%)','Rainfall(mm)', 'Snowfall (cm)']\n",
        "for i in columns :\n",
        "    plt.subplot(4,2,c)\n",
        "    plt.ylabel(i)\n",
        "    plt.title(label=i,fontsize=15,color=\"black\")\n",
        "    sns.barplot(data=bd_df, x='Seasons', y=i, palette='Set1')\n",
        "    sns.lineplot(data=bd_df, x='Seasons', y=i, color='black')\n",
        "    c = c + 1\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "vHDd_rWy8wYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   It is evident that the seasonal demand for rental bikes is positively associated with temperature, solar radiation ,rainfall ,humidity and is negatively related with Snowfall received.\n",
        "*   Therefore,the demand is highest during Summer season and least during winters\n",
        "\n"
      ],
      "metadata": {
        "id": "DgCrU2oy83Mp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Conclusions from Bivariate Analysis\n",
        "\n",
        "\n",
        "\n",
        "*   Temperature and Hour have a strong correlation with the count of rented bikes.\n",
        "\n",
        "* Dew point temperature is highly positively correlated to the Temperature.  \n",
        "* The peak demands for rental bikes occur on the opening (8-9 AM) and closing times (6-7pm) of offices and institutions\n",
        "\n",
        "*   During the period from Dec 2017 to Nov 2018,bike rental facilities were available on most days.The service was unavailable only for 13 days.\n",
        "\n",
        "*   The demand for rental bikes is higher on Regular days(Non-Holidays)\n",
        "*   There is more demand for rental bikes on Weekdays than on Weekends.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   There is a significant drop in the number of rented bikes during Winters(Dec-Feb) because it's freezing cold!\n",
        "\n",
        "\n",
        "*       The demand for bikes increases during warmer temperatures,which is why there's maximum count of rented bikes during the Summer season.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "auEP9Lq_9Blt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature engineering"
      ],
      "metadata": {
        "id": "tzgzo-7xdnhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for multicollinearity\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def check_vif(dataframe):\n",
        "  '''\n",
        "  This function calculates the variance inflation factor of the independent features in the datasdet\n",
        "  '''\n",
        "\n",
        "  # the independent variables set\n",
        "  X =dataframe\n",
        "  # VIF dataframe\n",
        "  vif_data = pd.DataFrame()\n",
        "  vif_data[\"feature\"] = X.columns\n",
        "\n",
        "  # calculating VIF for each feature\n",
        "  vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
        "                            for i in range(len(X.columns))]\n",
        "  print(vif_data)"
      ],
      "metadata": {
        "id": "rd3pp_9ddtRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the columns in the dataframe\n",
        "bd_df.columns"
      ],
      "metadata": {
        "id": "OwmtE0eldkuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the VIF value of certain columns in bd_df\n",
        "check_vif(bd_df[['Hour', 'Temperature(°C)', 'Humidity(%)',\n",
        "       'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(°C)',\n",
        "       'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)',\n",
        "        'Month','Year','Day']])"
      ],
      "metadata": {
        "id": "VcjjGp9qeg5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Multicolinearity causes reduction in the statistical power of your regression model\n",
        "\n",
        "Let's check the values of VIF if we exclude Dew point temperature and Year.\n"
      ],
      "metadata": {
        "id": "RbZB4WsGekfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_vif(bd_df[['Hour', 'Temperature(°C)', 'Humidity(%)','Wind speed (m/s)', 'Visibility (10m)',\n",
        "       'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)','Month','Day']])"
      ],
      "metadata": {
        "id": "2NvuaARheuoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The VIF of the features,now lie within the acceptable range."
      ],
      "metadata": {
        "id": "hLmkzX_newR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping 'Dew point temperature(°C)','Year' to reduce the VIF\n",
        "bd_df.drop(columns=['Dew point temperature(°C)','Year'],inplace=True)"
      ],
      "metadata": {
        "id": "_DeKFh44e1Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a copy of the main dataframe 'bd_df'\n",
        "df=bd_df.copy()"
      ],
      "metadata": {
        "id": "QvyVGZl0e7uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating dummies for the Categorical columns\n",
        "df = pd.get_dummies(bd_df, columns = ['Seasons','Holiday','Weekend','Functioning Day'],drop_first=True)\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "Q9uh9ONXfCJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "BAaBqkesfHDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping the columns Date and Days_of_week\n",
        "df.drop(['Days_of_week','Date'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "KdpM6ATzfI2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the columns present in the dataframe 'df'\n",
        "df.columns"
      ],
      "metadata": {
        "id": "vpdaig8jfM_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Regression Model"
      ],
      "metadata": {
        "id": "XRIjhHxQfRdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score"
      ],
      "metadata": {
        "id": "6HgvuzR5fZu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining independent and dependent variables\n",
        "\n",
        "y = df['Rented Bike Count']\n",
        "X = df.drop('Rented Bike Count',axis=1)"
      ],
      "metadata": {
        "id": "7XFHQAVdfgI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "lG23oZAXfn6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The shape of X : {X.shape}\\n The shape of X_train : {X_train.shape}\\n The shape of X_test : {X_test.shape}')"
      ],
      "metadata": {
        "id": "CkNO4s4ifpOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating functions to calculate the Evaluation metrics for the regression models\n",
        "\n",
        "def evaluate_model(name,X_test,y_true,y_pred):\n",
        "\n",
        "  '''\n",
        "  This function calculate  metrics for evaluating\n",
        "  the perfomance of Regression models\n",
        "  '''\n",
        "  list_=[]\n",
        "  #calculating mean absolute error\n",
        "  MAE =  mean_absolute_error(y_true,y_pred)\n",
        "  print(f'MAE : {MAE}')\n",
        "\n",
        "  #finding mean_squared_error\n",
        "  MSE  = mean_squared_error(y_true,y_pred)\n",
        "  print(\"MSE :\" , MSE)\n",
        "\n",
        "  #finding root mean squared error\n",
        "  RMSE = np.sqrt(MSE)\n",
        "  print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "  #finding the r2 score\n",
        "  r2 = r2_score(y_true,y_pred)\n",
        "  print(\"R2 :\" ,r2)\n",
        "\n",
        "  #finding the adjusted r2 score\n",
        "  adj_r2=1-(1-r2_score(y_true,y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "  print(\"Adjusted R2 : \",adj_r2)\n",
        "  list_.extend([name,MAE,MSE,RMSE,r2,adj_r2])\n",
        "  return(list_)"
      ],
      "metadata": {
        "id": "Ev8Llc4zf0ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a  list which would store lists of different models and their performance metrics\n",
        "list_of_models=[]"
      ],
      "metadata": {
        "id": "-T7-HWIKf4XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "oJMfJltSf-pN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple linear regression**"
      ],
      "metadata": {
        "id": "bGldT0LQgDcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaling the features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "std = StandardScaler()\n",
        "X_train = std.fit_transform(X_train)\n",
        "X_test = std.transform(X_test)"
      ],
      "metadata": {
        "id": "eQb7wkSjf9dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the Linear Regression model\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "9kx40U4TgTJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the data to Linear Regression model\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "HNr8DMPhgXRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting the values of y from X_test\n",
        "y_pred= regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "asJSwbIKgcV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the model\n",
        "list_of_models.append(evaluate_model('Multiple Linear Regression',X_test,y_test,y_pred))"
      ],
      "metadata": {
        "id": "xI-P7o6aghS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# *   Lasso Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "kclnsMHN4qI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the classes required for Cross Validation\n",
        "from sklearn.model_selection import RandomizedSearchCV as rscv\n",
        "from sklearn.model_selection import GridSearchCV as gsv"
      ],
      "metadata": {
        "id": "rijJ3TXu4lJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the linear_model class from sklearn library\n",
        "from sklearn import linear_model\n",
        "\n",
        "#Creating a Lasso Linear model object\n",
        "ls_model = linear_model.Lasso()"
      ],
      "metadata": {
        "id": "FhOF3VlG48J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the parameter grid\n",
        "grid = dict()\n",
        "grid['alpha'] = np.arange(0, 1, 0.005,)\n",
        "grid['max_iter'] = [25,50,100,500,1000]"
      ],
      "metadata": {
        "id": "vh3bALwN5TUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#performing GridSearch CV\n",
        "ls_model=gsv(estimator=ls_model, param_grid=grid,cv=5 ,verbose=1, scoring='r2')\n"
      ],
      "metadata": {
        "id": "1rgLluBm5XxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "ls_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "HxAfiDy05a5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#displaying the best estimators and score\n",
        "print(ls_model.best_estimator_,'The best score is ',ls_model.best_score_)"
      ],
      "metadata": {
        "id": "WSrWjkYx5ocY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the data to optimal Lasso model\n",
        "best_lasso = ls_model.best_estimator_\n",
        "best_lasso.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "4CyCWI9s5wa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediciting the values for y from X_test using the best parameters\n",
        "y_pred=best_lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "svn8-aRw_FYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the model\n",
        "list_of_models.append(evaluate_model('Lasso Regression(Tuned)',X_test,y_test,y_pred))"
      ],
      "metadata": {
        "id": "RGlkdQnc4gDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge Regression"
      ],
      "metadata": {
        "id": "neuGQDsqDGa3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Ridge regression with default parameters\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fREMc1UbDWG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing Ridge from linear_model class of sklearn library\n",
        "from sklearn.linear_model import Ridge"
      ],
      "metadata": {
        "id": "hd89Sh45DVP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an instance of Ridge regression\n",
        "ridge=Ridge()"
      ],
      "metadata": {
        "id": "fVy7U9OaD96_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "ridge.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "e1troO4CEAaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting the values of y from the test data\n",
        "y_pred=ridge.predict(X_test)\n",
        "\n",
        "ridge.score(X_train,y_train)"
      ],
      "metadata": {
        "id": "pKhTQqseEHkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the model\n",
        "list_of_models.append(evaluate_model('Ridge Regression (default)',X_test,y_test,y_pred))"
      ],
      "metadata": {
        "id": "93Ql4COBEU_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Ridge Regression with Hper Parameter Tuning\n",
        "\n"
      ],
      "metadata": {
        "id": "35iQ8CRaEjFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an object of linear model with Ridge regularization\n",
        "Ridge_model = linear_model.Ridge()"
      ],
      "metadata": {
        "id": "tuD73pTMEtj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the parameter grid\n",
        "grid = dict()\n",
        "grid['alpha'] = np.arange(0, 1, 0.005)\n",
        "grid['max_iter'] = [25,50,100,500,1000]"
      ],
      "metadata": {
        "id": "Z5Q59mIrExeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perfoming cross-validation to find the best model\n",
        "Ridge_model=gsv(estimator=Ridge_model, param_grid=grid,cv=5 ,verbose=1, scoring='r2')\n",
        "Ridge_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "ovVT1pY9E0iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Ridge_model.best_estimator_,Ridge_model.best_score_)"
      ],
      "metadata": {
        "id": "-1x2kWWPE418"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the train data to the Ridge model with best parameters\n",
        "best_ridge = Ridge_model.best_estimator_\n",
        "best_ridge.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "T-7quasRE8yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_ridge.predict(X_test)"
      ],
      "metadata": {
        "id": "2IT8-DxUFAaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the model\n",
        "list_of_models.append(evaluate_model('Ridge Regression(Tuned)',X_test,y_test,y_pred))"
      ],
      "metadata": {
        "id": "0zBpKwe9FDlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elasticnet Regression"
      ],
      "metadata": {
        "id": "o4lN3i7rFGCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Elastic Net\n",
        "from sklearn.linear_model import ElasticNet"
      ],
      "metadata": {
        "id": "jJxxOmBbFPQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an instance of ElasticNet model\n",
        "elasticnet = ElasticNet()"
      ],
      "metadata": {
        "id": "o_o1k-32FSQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the model to train data and finding its score\n",
        "elasticnet.fit(X_train,y_train)\n",
        "elasticnet.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "ftST3vyMFWWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting the values of y from test data\n",
        "y_pred = elasticnet.predict(X_test)"
      ],
      "metadata": {
        "id": "rfVBQ7b3FaJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the model\n",
        "list_of_models.append(evaluate_model('Elastic Net Regression(default)',X_test,y_test,y_pred))"
      ],
      "metadata": {
        "id": "WFjAWeWXFdBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Elasticnet Regression with Hyperparameter Tuning\n",
        "\n"
      ],
      "metadata": {
        "id": "QrwDRRYMFfYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an instance of ElasticNet model\n",
        "elastic = ElasticNet()\n",
        "\n",
        "#Creating the parameter grid\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100],'l1_ratio':[0.3,0.4,0.5,0.6,0.7,0.8]}"
      ],
      "metadata": {
        "id": "umVyiAuvFqV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Performing GridSearch Cross Validation to find the best parameters\n",
        "\n",
        "elastic_regressor = gsv(elastic, parameters,scoring='neg_mean_squared_error',cv=5)\n",
        "elastic_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "I3veIbXLFumG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,elastic_regressor.best_params_)\n",
        "print(\"\\nUsing \",elastic_regressor.best_params_, \" the negative mean squared error is: \", elastic_regressor.best_score_)"
      ],
      "metadata": {
        "id": "x13UR5pwFzYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_elasticnet=elastic_regressor.best_estimator_\n",
        "best_elasticnet.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "liIL10_6F3Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting the values of y from best model\n",
        "y_pred_elastic = best_elasticnet.predict(X_test)"
      ],
      "metadata": {
        "id": "aMiQtIz7F6TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the model\n",
        "list_of_models.append(evaluate_model('Elastic Net Regression(Tuned)',X_test,y_test,y_pred_elastic))"
      ],
      "metadata": {
        "id": "0Fr7othPF-rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial Regression"
      ],
      "metadata": {
        "id": "jtCJDdJYGH0J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDdDZlArtTtQ"
      },
      "outputs": [],
      "source": [
        "#importing the packages required for preprocessing,creating pipeline,cross-validation\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def PolynomialRegression(degree=1, **kwargs):#initializing the degree as 1,however the degree will change while performing GridSearch cross-validation\n",
        "  '''\n",
        "  This function transforms the independent features(X_train) to a polynomial of the degree given in the parameters\n",
        "  and performs Linear regression using the  y_train(not-transformed) and the transformed X_train.\n",
        "  '''\n",
        "  return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))"
      ],
      "metadata": {
        "id": "6cFRJ0MTGW-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a parameter grid\n",
        "parameters = {'polynomialfeatures__degree': [2,3,4,5]}\n",
        "\n",
        "#Creating a PolynomialRegression object\n",
        "poly_regressor = PolynomialRegression()\n",
        "\n",
        "#Perfoming GridSearch Cross-validation to find the optimal parameters\n",
        "poly_grid = GridSearchCV(poly_regressor, param_grid=parameters,cv=3, scoring='neg_mean_squared_error', verbose=3)\n",
        "poly_grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "qa7RRYclGbyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n The best parameters across ALL searched params:\\n\",poly_grid.best_params_)"
      ],
      "metadata": {
        "id": "yRzyyYg1GfqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the independent features to a polynomial of degree 2\n",
        "poly_features = PolynomialFeatures(degree =2)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "\n",
        "#Performing Linear regression using  y_train and thetransformed X_train\n",
        "poly_regressor = LinearRegression( )\n",
        "poly_regressor.fit(X_train_poly, y_train)\n",
        "\n",
        "#Predicting the y values from X_test\n",
        "\n",
        "X_test_transform=poly_features.transform(X_test)\n",
        "y_pred=poly_regressor.predict(X_test_transform)"
      ],
      "metadata": {
        "id": "SFY-Q7-cGjFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Determining the evaluation metrics of the model\n",
        "print(f'The polynomial with degree = 2 is optimal fit')\n",
        "list_of_models.append(evaluate_model('Polynomial Regression(Tuned)',X_test,y_test,y_pred))"
      ],
      "metadata": {
        "id": "mResejKpGm9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Regressor"
      ],
      "metadata": {
        "id": "vNjMogQsGoUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the data to train and test again(to obtain non-scaled test and train data)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "k-8QtwsUGtgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making function to plot important features.\n",
        "def plotting_imp_features(model, training_data):\n",
        "  imp_features = model.feature_importances_\n",
        "  feature_names = training_data.columns\n",
        "  _imp_features = pd.Series(imp_features, index=feature_names)\n",
        "  return _imp_features.sort_values(ascending=True).plot(kind='barh',figsize=[10,8], title='Feature Importance')"
      ],
      "metadata": {
        "id": "aRF-YZTVGxFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "#Creating a decision tree regressor object\n",
        "dtr = DecisionTreeRegressor()\n",
        "\n",
        "# hyper-parameter tuning using gridSearchCV\n",
        "parameters = {'max_depth': [int(i) for i in np.linspace(start=3, stop=20, num=17)],\n",
        "              'min_samples_split':[2,3,4],\n",
        "              'min_samples_leaf':[1,2,3,4],\n",
        "              }\n",
        "\n",
        "gridsearch_dtr = GridSearchCV(dtr, parameters, scoring='r2', cv=5)\n",
        "gridsearch_dtr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "JTlYyFIUG2IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best parameters.\n",
        "print('Best parameters for our model are: max_depth={}, min_samples_split={}, min_samples_leaf={}'.format(gridsearch_dtr.best_params_['max_depth'],\n",
        "                                    gridsearch_dtr.best_params_['min_samples_split'], gridsearch_dtr.best_params_['min_samples_leaf']))"
      ],
      "metadata": {
        "id": "iul2NdM3G5H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test performance\n",
        "train_score = gridsearch_dtr.best_score_\n",
        "test_score = gridsearch_dtr.best_estimator_.score(X_test,y_test)\n",
        "\n",
        "print('The best r2 score for train data is {}'.format(train_score))\n",
        "print('The best r2 score for test data is {}'.format(test_score))"
      ],
      "metadata": {
        "id": "CbhZW2HnG-Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the data to the Decision tree regressor with tuned parameters\n",
        "best_dtr = gridsearch_dtr.best_estimator_"
      ],
      "metadata": {
        "id": "1J1lYAAGHA1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_dtr.fit(X_train,y_train)\n",
        "#Making predictions on the best decision tree model\n",
        "y_pred=best_dtr.predict(X_test)"
      ],
      "metadata": {
        "id": "KhUpGr7FHDie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the model\n",
        "list_of_models.append(evaluate_model('Decision Tree Regression (Tuned)',X_test,y_test,y_pred))"
      ],
      "metadata": {
        "id": "enPovUHuHHpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing features importance of Decision Tree Regressor model\n",
        "plotting_imp_features(gridsearch_dtr.best_estimator_, X_train)"
      ],
      "metadata": {
        "id": "oQ37uCsfHSr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost Regression"
      ],
      "metadata": {
        "id": "as9EyQdSedeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the required packages and classes\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "X5_SWuzveojC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse_cv(model):\n",
        "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5)).mean()\n",
        "    return rmse\n",
        "\n",
        "def evaluation(y, predictions):\n",
        "    mae = mean_absolute_error(y, predictions)\n",
        "    mse = mean_squared_error(y, predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(y, predictions))\n",
        "    r_squared = r2_score(y, predictions)\n",
        "    return mae, mse, rmse, r_squared\n"
      ],
      "metadata": {
        "id": "WHaxIaNCet9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xgboost with default parameters"
      ],
      "metadata": {
        "id": "o8bzm6xHe3zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = ['Seasons', 'Holiday', 'Functioning Day', 'Weekend']\n",
        "\n",
        "# Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(), categorical_cols)\n",
        "    ], remainder='passthrough'  # Keep non-categorical columns as they are\n",
        ")\n",
        "\n",
        "# Create an XGBoost Regressor instance with specified hyperparameters\n",
        "xgb = XGBRegressor(n_estimators=1000, learning_rate=0.01)\n",
        "\n",
        "# Create a pipeline that includes preprocessing and the XGBoost Regressor\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('xgb', xgb)\n",
        "])\n",
        "\n",
        "# Fit the model to the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "rmse = mean_squared_error(y_test, predictions, squared=False)  # Calculate RMSE\n",
        "r_squared = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MAE:\", mae)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R2 Score:\", r_squared)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Calculate RMSE using cross-validation (You need to define the function rmse_cv)\n",
        "rmse_cross_val = rmse_cv(pipeline)  # Define the rmse_cv function to perform cross-validation\n",
        "print(\"RMSE Cross-Validation:\", rmse_cross_val)"
      ],
      "metadata": {
        "id": "Nlxq30z-awGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xgboost with hyperparameter Tuning\n"
      ],
      "metadata": {
        "id": "gO4Kqg-yfHUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "# Create an XGBoost Regressor instance\n",
        "xgb = XGBRegressor()\n",
        "\n",
        "# Create a pipeline that includes preprocessing and the XGBoost Regressor\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('xgb', xgb)\n",
        "])\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_dist = {\n",
        "    'xgb__n_estimators': stats.randint(100, 1000),  # Random integer between 100 and 1000\n",
        "    'xgb__learning_rate': stats.uniform(0.01, 0.2),  # Random float between 0.01 and 0.2\n",
        "    'xgb__max_depth': stats.randint(3, 6),  # Random integer between 3 and 5\n",
        "    'xgb__min_child_weight': stats.randint(1, 4)  # Random integer between 1 and 3\n",
        "}\n",
        "# Create a RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=50, cv=5,\n",
        "                                   scoring='neg_mean_squared_error', verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the random search to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "# Get the best model from the random search\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate RMSE and R-squared on the test set\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test RMSE:\", rmse)\n",
        "print(\"Test R-squared:\", r2)\n",
        "\n"
      ],
      "metadata": {
        "id": "PLuaXO8UBRgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an XGBoost Regressor instance with specific hyperparameters\n",
        "best_xgb = XGBRegressor(n_estimators=1000, learning_rate=0.03, subsample=0.7, objective='reg:squarederror',\n",
        "                        max_depth=7, silent=1, min_child_weight=4, colsample_bytree=0.7)\n",
        "\n",
        "# Create a pipeline that includes preprocessing and the XGBoost Regressor\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('xgb', best_xgb)\n",
        "])\n",
        "\n",
        "# Fit the model to the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate and print the training score (R-squared)\n",
        "training_score = pipeline.score(X_train, y_train)\n",
        "print(f'Training score: {training_score}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_r2_score = r2_score(y_test, y_pred)\n",
        "print(\"Test R-squared Score:\", test_r2_score)"
      ],
      "metadata": {
        "id": "YqCGouKQLgms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plotting_imp_features(model, X, importance_type='weight', max_num_features=None, title='Feature Importance'):\n",
        "    # Get feature importance scores using XGBoost's built-in method\n",
        "    importance_scores = model.get_booster().get_score(importance_type=importance_type)\n",
        "\n",
        "    # Extract feature names and importance scores\n",
        "    feature_names, scores = zip(*importance_scores.items())\n",
        "\n",
        "    # Create a DataFrame to store feature names and their importance scores\n",
        "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': scores})\n",
        "\n",
        "    # Sort features by importance score in descending order\n",
        "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Select the top features if max_num_features is specified\n",
        "    if max_num_features is not None:\n",
        "        feature_importance_df = feature_importance_df.head(max_num_features)\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "    plt.xlabel('Importance')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.title(title)\n",
        "    plt.gca().invert_yaxis()  # Invert the y-axis to show the most important feature at the top\n",
        "    plt.show()\n",
        "\n",
        "# Call the plotting_imp_features function with your trained XGBoost model and X_train\n",
        "plotting_imp_features(best_xgb, X_train, max_num_features=10)"
      ],
      "metadata": {
        "id": "o2R9O0wMM9cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CATBoost Regressor"
      ],
      "metadata": {
        "id": "-2GeHXqTfm9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installing CatBoost package\n",
        "!pip install catboost"
      ],
      "metadata": {
        "id": "6Rcj9YIkfhWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the Catboost Regressor class\n",
        "from catboost import CatBoostRegressor"
      ],
      "metadata": {
        "id": "1HBaWspwfw9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the list of columns in the main data frame\n",
        "bd_df.columns"
      ],
      "metadata": {
        "id": "5wSawoGWfzl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=bd_df.drop(columns=['Date','Rented Bike Count','Days_of_week'])\n",
        "y=bd_df['Rented Bike Count']"
      ],
      "metadata": {
        "id": "5J-m0YiIf4eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List of categorical columns\n",
        "categorical_columns = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "print(\"Names of categorical columns : \", categorical_columns)\n",
        "\n",
        "#Get location of categorical columns\n",
        "categorical_features_indices = [X.columns.get_loc(col) for col in categorical_columns]\n",
        "print(\"Location of categorical columns : \",categorical_features_indices)"
      ],
      "metadata": {
        "id": "iM_cEgARf8g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the data to train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "IegwIBVkgBK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CATBoost Regressor with default parameters\n"
      ],
      "metadata": {
        "id": "0X-ogr3xgFf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating Catboost model\n",
        "CB_regressor= CatBoostRegressor( loss_function='RMSE')\n",
        "\n",
        " # train the model\n",
        "CB_regressor.fit(X_train,y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test),verbose=False,plot=False)"
      ],
      "metadata": {
        "id": "8NsJqY7sgL9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make predictions and evalution of Catboost model\n",
        "\n",
        "print(f'Training score:{CB_regressor.score(X_train,y_train)}')\n",
        "\n",
        "y_pred=CB_regressor.predict(X_test)\n",
        "\n",
        "#Evaluating the model\n",
        "list_of_models.append(evaluate_model('Catboost Regression(default)',X_test,y_test,y_pred))"
      ],
      "metadata": {
        "id": "zaGibcMXgQep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing features importance of Catboost model\n",
        "plotting_imp_features(CB_regressor, X_train)"
      ],
      "metadata": {
        "id": "gV51wwKugaHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CATBoost with Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "PqgJGyi0gh3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the optimal parameters by Grid Search Cross Validation\n",
        "\n",
        "parameters = {'depth' : [8,7,6],'learning_rate' : [0.025, 0.05, 0.1],'iterations':[100,200,500]}\n",
        "CB_regressor = CatBoostRegressor(iterations=50, loss_function='RMSE',cat_features=categorical_features_indices)\n",
        "grid = GridSearchCV(estimator=CB_regressor, param_grid = parameters, cv = 5, n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"\\n The best parameters across ALL searched params:\\n\",grid.best_params_)\n"
      ],
      "metadata": {
        "id": "vdW9tcDdgw2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating Catboost model\n",
        "best_CB_regressor= CatBoostRegressor(iterations=500,depth=8,learning_rate=0.1, loss_function='RMSE')\n",
        " # train the model\n",
        "best_CB_regressor.fit(X_train,y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test),verbose=False,plot=False)"
      ],
      "metadata": {
        "id": "r5PfpiFIg5Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make predictions and evalution of Catboost model\n",
        "\n",
        "print(f'Training score:{best_CB_regressor.score(X_train,y_train)}')\n",
        "\n",
        "y_pred=best_CB_regressor.predict(X_test)\n",
        "\n",
        "#Evaluating the model\n",
        "list_of_models.append(evaluate_model('Catboost Regression(tuned)',X_test,y_test,y_pred))"
      ],
      "metadata": {
        "id": "peYRafs8g59D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing features importance of the best Catboost model\n",
        "plotting_imp_features(best_CB_regressor, X_train)"
      ],
      "metadata": {
        "id": "V26TklYqg-Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparing models\n",
        "Comparison_df=pd.DataFrame(list_of_models,columns=['Regression Model','Mean Absolute Error','Mean Squared Error','Root Mean Squared Error','r2 score','adjusted r2 score'])\n",
        "Comparison_df"
      ],
      "metadata": {
        "id": "OWz2WAtehEF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "atkO_ndphMYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  \n",
        "\n",
        "    Evaluating the performance metrics of the models has brought us to a conclusion that Decison tree based Ensemble models like XGBoost and CatBoost models are the most suitable for Predicting the number of bikes required on an hourly basis.\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "*   The important features for prediction are : Hour &Temperature.\n",
        "\n",
        "*   Due to the lack of significant linear correlation between the independent variables and the count of Rented bikes,Linear regression and Polynomial regression are not good fit in this scenario.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XEA6kydehPjz"
      }
    }
  ]
}